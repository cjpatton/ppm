



TODO Working Group                                             C. Patton
Internet-Draft                                                Cloudflare
Intended status: Informational                         21 September 2021
Expires: 25 March 2022


        Multi-party Protocols for Privacy-preserving Measurement
                     draft-patton-multi-ppm-latest

Abstract

   [TODO Move and rename this document to "draft-patton-vdaf".]  This
   document describes Verifiable Distributed Aggregation Functions
   (VDAFs), a family of multi-party protocols for computing aggregate
   statistics over user measurements.  These protocols are designed to
   ensure that, as long as at least one aggregation server executes the
   protocol honestly, individual measurements are never seen by any
   server in the clear.  At the same time, VDAFs allow the servers to
   detect if a misconfigured or malicious client submitted a malformed
   input.

Discussion Venues

   This note is to be removed before publishing as an RFC.

   Discussion of this document takes place on the mailing list (), which
   is archived at .

   Source for this draft and an issue tracker can be found at
   https://github.com/cjpatton/ppm.

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at https://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on 25 March 2022.

Copyright Notice

   Copyright (c) 2021 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents (https://trustee.ietf.org/
   license-info) in effect on the date of publication of this document.
   Please review these documents carefully, as they describe your rights
   and restrictions with respect to this document.  Code Components
   extracted from this document must include Simplified BSD License text
   as described in Section 4.e of the Trust Legal Provisions and are
   provided without warranty as described in the Simplified BSD License.

Table of Contents

   1.  Introduction
   2.  Conventions and Definitions
   3.  Distributed Aggregation Functions
     3.1.  Aggregability
   4.  Verifiable Distributed Aggregation Functions
   5.  prio3
     5.1.  Dependencies
       5.1.1.  Fully Linear, Probabilistically Checkable Proof
       5.1.2.  Key Derivation
     5.2.  Construction
   6.  Security Considerations
   7.  IANA Considerations
   8.  References
     8.1.  Normative References
     8.2.  Informative References
   Acknowledgments
   Author's Address

1.  Introduction

   TODO Introduction

   The etymology of the term "VDAF" is that it is a generalization of
   "Distributed Point Functions (DPFs)" [GI14].

   VDAFs from the literature:

   *  Prio [CGB17] defines the composition of a linear secret sharing
      scheme and an affine-aggregatable encoding of a statistic.

   *  A special case of zero-knowledge proofs over distributed data
      [BBDGGI19] in which the client speaks once.

   *  The composition of an incremental distributed point function and
      the secure-sketching protocol for subset histograms defined in
      [BBDGGI21].

   *  Prio+ [AGJOP21] has the client upload XOR shares and then has the
      servers convert them to additive shares over a number of rounds.

   This document is structured as follows.

   *  Section 3 defines Distributed Aggregation Functions (DAFs), which
      distribute the computation of an aggregation function among a set
      of aggregators in order to keep the inputs private.

   *  Section 4 defines Verifiable Distributed Aggregation Functions
      (VDAFs), an extension of DAFs that additionally allow the
      aggregators to detect malformed inputs.

2.  Conventions and Definitions

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in
   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all
   capitals, as shown here.

   Algorithms are written in Python 3.  Unless noted otherwise, function
   parameters without a type hint have type "bytes".  A fatal error in a
   program (e.g., failure to parse one of the function parameters) is
   usually handled by raising an exception.

3.  Distributed Aggregation Functions

   client
     | input
     v
   +-----------------------------------------------------------+
   | daf_input()                                               |
   +-----------------------------------------------------------+
     | input_shares[1]  | input_shares[2]   ...  | input_shares[SHARES]
     v                  v                        v
   +---------------+  +---------------+        +---------------+
   | daf_output()  |  | daf_output()  |        | daf_output()  |
   +---------------+  +---------------+        +---------------+
     | output_shares[1] | output_shares[2]  ...  | output_shares[SHARES]
     v                  v                        v
   aggregator 1       aggregator 2             aggregator SHARES

                       Figure 1: Execution of a DAF.

   A DAF is a multi-party protocol for executing an aggregation function
   over a set of user inputs.  By distributing the input across multiple
   aggregators, the protocol ensures that individual inputs are never
   seen in the clear.  Syntactically, an DAF is made up of two
   algorithms:

   *  "daf_input(input) -> input_shares" is the randomized input-
      distribution algorithm.  It is run by the client in order to split
      its input into "SHARES" input shares (i.e., "len(input_shares) ==
      s").  Each input share is sent to one of the aggregators.

   *  "daf_output(param, input_share) -> output_share" is the
      deterministic output-recovery algorithm.  It is run be each
      aggregator in order to map an input share to an output share.
      This mapping has a parameter "param", which can be used to "query"
      the input share multiple times with multiple parameters, getting a
      different output share each time. "param" is called the
      aggregation parameter.

   Execution of a DAF is illustrated in Figure 1.  The client runs the
   input-distribution algorithm and sends an input share to each one of
   the aggregators.  Next, the aggregators select a parameter for
   querying the input shares, and each runs the output-recover algorithm
   to obtain their share of the output.  DAF schemes are designed to
   ensure that no proper subset of the aggregators can discern any
   information about the input or output given their view of the
   protocol.  (See Section 6.)

   Associated constants:

   *  "SHARES" is the number of aggregators for which the DAF is
      defined.

3.1.  Aggregability

   Let "G(param)" denote the support of the output-recovery algorithm
   for a given aggregation parameter "param".  That is, set "G(param)"
   contains the set of all possible outputs of "daf_output" when the
   first input is "param" and the second is any input share.

   Correctness requires that, for every aggregation parameter "param",
   the set "G(param)" forms an additive group.  This allows the
   aggregation function to be computed by having each aggregator sum up
   its output shares locally, then collectively computing the output by
   summing up their aggregated output shares.  In particular, the
   aggregation function is computed by the following algorithm. (let
   "Zero[param]" be the identity element of "G(param)"):

   def run_daf(param, inputs):
     output_shares = [ Zero[param] for _ in range(SHARES) ]

     for input in inputs:
       # Each client runs the input-distribution algorithm.
       input_shares = daf_input(input)

       # Each aggregator runs the output-recvoery algorithm.
       for j in range(SHARES):
         output_shares[j] += daf_output(param, input_shares[j])

     # Aggregators compute the final output.
     return sum(output_shares)

      Figure 2: Definition of the aggregation function computed by an
                             s-aggregator DAF.

4.  Verifiable Distributed Aggregation Functions

   client
     | input
     v
   +-----------------------------------------------------------+
   | vdaf_input()                                              |
   +-----------------------------------------------------------+
     | input_shares[1]  | input_shares[2]   ...  | input_shares[SHARES]
     v                  v                        v
   +---------------+  +---------------+        +---------------+
   | vdaf_start()  |  | vdaf_start()  |        | vdaf_start()  |
   +---------------+  +---------------+        +---------------+
     |                  |                   ...  |
     =============================================
     |                  |                        |
     v                  v                        v
   +---------------+  +---------------+        +---------------+
   | vdaf_next_2() |  | vdaf_next_2() |        | vdaf_next_2() |
   +---------------+  +---------------+        +---------------+
     |                  |                   ...  |
     =============================================
     |                  |                        |
     v                  v                        v
     .                  .                        .
     .                  .                        .
     .                  .                        .
     |                  |                        |
     v                  v                        v
   +---------------+  +---------------+        +---------------+
   | vdaf_finish() |  | vdaf_finish() |        | vdaf_finish() |
   +---------------+  +---------------+        +---------------+
     | output_shares[1] | output_shares[2]  ...  | output_shares[SHARES]
     v                  v                        v
   aggregator 1       aggregator 2             aggregator SHARES

         Figure 3: Execution of a VDAF.  The === line represents a
                             broadcast channel.

   The main limitation of DAF schemes is that, because each aggregator
   only holds a piece of the distributed input, there is no way for them
   to check that the output is valid.  A VDAF is an extension of a DAF
   in which the aggregators verify that the output is valid before
   recovering their output shares, without leaking their shares to the
   other aggregators.  Doing so requires the aggregators to interact
   with one another, which they do over a broadcast channel.

   Execution of a VDAF is illustrated in Figure 3.  It begins just as
   before (see Figure 1) by having the client run the input-distribution
   algorithm and send an input share to each of the aggregators.  The
   aggregators then proceed in a constant number of rounds, where in
   each round, each aggregator produces a single outbound message.  The
   outbound messages are written to a broadcast channel, which transmits
   all of the messages to each aggregator in the next round.
   Eventually, each aggregator decides if the input shares are valid
   based on its view of the protocol.  If so, it returns an output
   share.  Otherwise it returns an indication of invalidity.

   Syntactically, a VDAF is made up of the following algorithms:

   *  "vdaf_input(input) -> input_shares" is the input-distribution
      algorithm defined precisely the same way as "daf_input" in
      Section 3.

   *  "vdaf_init(param) -> states: Vec[State]" is the state-
      initialization algorithm.  It takes as input the aggregation
      parameter and outputs the initial state of each aggregator (i.e.,
      "len(states) == SHARES").  This algorithm is executed out-of-band
      and is used to configure the aggregators with whatever they need
      to run the protocol (e.g., shared randomness).  Type "State" is
      defined by the VDAF scheme.

   *  "vdaf_start(state: State, input_share) -> (new_state: State,
      outbound_message)" is the verify-start algorithm and is run by
      each aggregator in response to an input share from the client.
      Its output is the aggregator's first outbound message to be
      broadcast to the other aggregators.

   *  "vdaf_next_i(state: State, inbound_messages) -> (new_state: State,
      outbound_message)" consumes the "(i-1)"-th round of inbound
      messages (note that "len(inbound_messages) == SHARES") and
      produces the aggregator's "i"-th outbound message.  The protocol
      specifies such a function for every "2 <= i <= ROUNDS", where
      "ROUNDS" is the number of rounds of the protocol.  If "ROUNDS ==
      1", then no such function is not defined.

   *  "vdaf_finish(state: State, inbound_messages) -> output_share" is
      the verify-finish algorithm.  It consumes the last round of
      inbound messages (note that "len(inbound_messages) == SHARES") and
      produces the aggregator's output share, or an indication that the
      input shares are invalid.

   Associated types:

   *  "State" is the state of an aggregator during executing of the
      VDAF.  It is defined by the VDAF itself.

   Associated constants:

   *  "SHARES" is the number of aggregators for which the VDAF is
      defined.

   *  "ROUNDS" is the number of rounds of communication between the
      aggregators.

   Just as for DAF schemes, we require that for each aggregation
   parameter "param", the set of output shares "G(param)" forms an
   additive group.  The aggregation function is computed by running the
   VDAF as specified below (let "Zero[param]" denote the additive
   identity of "G(param)"):

   def run_vdaf(param, inputs):
     output_shares = [ Zero[param] for _ in range(SHARES) ]

     for input in inputs:
       # Each client runs the input-distribution algorithm.
       input_shares = vdaf_input(input)

       # Aggregators verify and recover their output shares.
       states = vdaf_init(param)

       outbound = []
       for j in range(SHARES):
         (states[j], msg) = vdaf_start(states[j], input_shares[j])
         outbound.append(msg)
       inbound = outbound

       for i in range(ROUNDS-1):
         outbound = []
         for j in range(SHARES):
           (states[j], msg) = vdaf_next_i(states[j], inbound)
           outbound.append(msg)
         inbound = outbound

       for j in range(SHARES):
         output_share[j] += vdaf_finish(states[j], inbound)

     # Aggregators compute the final output.
     return sum(output_shares)

                       Figure 4: Execution of a VDAF.

5.  prio3

   NOTE This is WIP.

   NOTE This protocol has not undergone significant security analysis.
   This is planned for [PAPER].

   The etymology of the term "prio3" is that it descends from the
   original Prio construction [CGB17], which was deployed in Firefox's
   origin telemetry project (i.e., "prio1").  It generalizes the more
   recent deployment in the ENPA system (i.e., "prio2") and is based on
   techniques described in [BBDGGI19].

5.1.  Dependencies

5.1.1.  Fully Linear, Probabilistically Checkable Proof

   NOTE [BBDGGI19] call this a 1.5-round, public-coin, interactive
   oracle proof system.

   All raise "ERR_INPUT":

   *  "pcp_prove(input: Vec[Field], prove_rand: Vec[Field], joint_rand:
      Vec[Field]) -> proof: Vec[Field]" is the proof-generation
      algorithm.

   *  "pcp_query(input: Vec[Field], proof: Vec[Field], query_rand:
      Vec[Field], joint_rand: Vec[Field]) -> verifier: Vec[Field]" is
      the query-generation algorithm.

   *  "pcp_decide(verifier: Vec[Field]) -> decision: Boolean" is the
      decision algorithm.

   Associated types

   *  "Field" Associated functions:

      -  "vec_rand(len: U32) -> output: Vec[Field]" require that "len ==
         len(output)"

      -  "vec_zeros(len: U32) -> output: Vec[Field]" require that "len
         == len(output)" and each element of "output" is zero.

      -  "encode_vec"

      -  "decode_vec" raises "ERR_DECODE"

      Associated constants:

      -  "ENCODED_SIZE"

   Associated constants:

   *  "JOINT_RAND_LEN"

   *  "PROVE_RAND_LEN"

   *  "QUERY_RAND_LEN"

   Execution semantics:

   def run_pcp(input: Vec[Field]):
     joint_rand = vec_rand(JOINT_RAND_LEN)
     prove_rand = vec_rand(PROVE_RAND_LEN)
     query_rand = vec_rand(QUERY_RAND_LEN)

     # Prover generates the proof.
     proof = pcp_prove(input, prove_rand, joint_rand)

     # Verifier queries the input and proof.
     verifier = pcp_query(input, proof, query_rand, joint_rand)

     # Verifier decides if the input is valid.
     return pcp_decide(verifier)

                 Figure 5: Execution of a fully linear PCP.

5.1.2.  Key Derivation

   [TODO Separate this syntax from what people usually think of as a
   KDF.]  A key-derivation scheme consists of the following algorithms:

   *  "get_key(init_key, input) -> key" require "len(init_key) ==
      KEY_SIZE" and "len(key) == KEY_SIZE".

   *  "get_key_stream(key) -> state: KeyStream" require that "len(key)
      == KEY_SIZE".

   *  "key_stream_next(state: KeyStream, length: U32) -> (new_state:
      KeyStream, output)" require that "length == len(output)"

   Associated types:

   *  "KeyStream"

   Associated constants:

   *  "KEY_SIZE"

5.2.  Construction

   def vdaf_input(r_input):
     input = decode_vec(r_input)
     k_joint_rand = zeros(SEED_SIZE)

     # Generate input shares.
     leader_input_share = input
     k_helper_input_shares = []
     k_helper_blinds = []
     k_helper_hints = []
     for j in range(SHARES-1):
       k_blind = get_rand(KEY_SIZE)
       k_share = get_rand(KEY_SIZE)
       helper_input_share = expand(k_share, len(leader_input_share))
       leader_input_share -= helper_input_share
       k_hint = get_key(k_blind,
           byte(j+1) + encode_vec(helper_input_share))
       k_joint_rand ^= k_hint
       k_helper_input_shares.append(k_share)
       k_helper_blinds.append(k_blind)
       k_helper_hints.append(k_hint)
     k_leader_blind = get_rand(KEY_SIZE)
     k_leader_hint = get_key(k_leader_blind,
         byte(0) + encode_vec(leader_input_share))
     k_joint_rand ^= k_leader_hint

     # Finish joint randomness.
     if JOINT_RAND_LEN > 0:
       for j in range(SHARES-1):
         k_helper_hints[i] ^= k_joint_rand
       k_leader_hint ^= k_joint_rand
       joint_rand = expand(k_joint_rand, JOINT_RAND_LEN)
     else:
       for j in range(SHARES-1):
         k_helper_hints[i] = None
       k_leader_hint = None
       joint_rand = []

     # Generate the proof shares.
     prove_rand = expand(get_rand(KEY_SIZE), PROVE_RAND_LEN)
     leader_proof_share = pcp_prove(input, prove_rand, joint_rand)
     k_helper_proof_shares = []
     for j in range(SHARES-1):
       k_share = get_rand(KEY_SIZE)
       k_helper_proof_shares.append(k_share)
       helper_proof_share = expand(k_share, len(leader_proof_share))
       leader_proof_share -= helper_proof_share

     output = []
     output.append(encode_leader_share(
       leader_input_share,
       leader_proof_share,
       k_leader_blind,
       k_leader_hint,
     ))
     for j in range(SHARES-1):
       output.append(encode_helper_share(
         (k_helper_input_share[j], len(leader_input_share)),
         (k_helper_proof_share[j], len(leader_proof_share)),
         k_helper_blinds[j],
         k_helper_hints[j],
       ))
     return output

          Figure 6: Input distribution algorithm for prio3.  TODO
            Figure out how this looks in the normal text format.

   def vdaf_init(ignored_param):
     k_query_rand = get_rand(KEY_SIZE)
     states = []
     for j in range(SHARES):
       states.append(ready(j, k_query_rand))
     return states

            Figure 7: State-initialization algorithm for prio3.

   def vdaf_start(state: State, r_input_share):
     if not state.ready(): raise ERR_STATE

     if state.aggregator_id == byte(0): # Leader
       (input_share,
        proof_share,
        k_blind,
        k_hint) = decode_leader_share(input_share)
     else: # Helper
       (l_input_share,
        l_proof_share,
        k_blind,
        k_hint) = decode_helper_share(input_share)
       input_share = expand(*l_input_share)
       proof_share = expand(*l_proof_share)

     if JOINT_RAND_LEN > 0:
       k_joint_rand_share = get_key(k_blind,
           aggregator_id + input_share)
       k_joint_rand = k_hint ^ k_joint_rand_share
       joint_rand = expand(k_joint_rand, JOINT_RAND_LEN)
     else:
       joint_rand = []

     query_rand = expand(state.k_query_rand, QUERY_RAND_LEN)
     verifier_share = pcp_query(
         input_share, proof_share, query_rand, joint_rand)
     verifier_length = len(verifier_share)

     new_state = wait(k_joint_rand, input_share, verifier_length)
     output = encode_verifier_share(k_joint_rand, verifier_share)
     return (new_state, output)

                Figure 8: Verify-start algorithm for prio3.

   def vdaf_finish(state: State, r_verifier_shares):
     if not state.waiting(): raise ERR_STATE
     if len(r_verifier_shares) != s: raise ERR_DECODE

     k_joint_rand = zeros(KEY_SIZE)
     verifier = vec_zeros(state.verifier_len)
     for r_share in r_verifier_shares:
       (k_joint_rand_share,
        verifier_share) = decode_verifier_share(r_share)
       if len(verifier_share) != state.verifier_length:
         raise ERR_DECODE

       k_joint_rand ^= k_joint_rand_share
       verifer += verifier_share

     if k_joint_rand != state.k_joint_rand: raise ERR_INVALID
     if not pcp_decide(verifier): raise ERR_INVALID
     return state.input_share

                Figure 9: Verify-finish algorithm for prio3.

   Auxiliary functions:

   *  "expand(seed, length: U32) -> output: Vec[Field]"

   *  "ready"

   *  "waiting"

   *  "encode_helper_share"

   *  "decode_helper_share" raises "ERR_DECODE"

   *  "encode_leader_share"

   *  "decode_leader_share" raises "ERR_DECODE"

   *  "encode_verifier_share"

   *  "decode_verifier_share" raises "ERR_DECODE"

   *  "get_rand(length: U32) -> output" require that "length ==
      len(output)"

   *  "zeros(length: U32) -> output" require that "lengh == len(output)"
      and that each element of "output" is zero.

6.  Security Considerations

   TODO There will be a companion paper [PAPER] that will formalize the
   syntax and security of VDAFs and analyze some of the constructions
   specified here.  Here we will say at a high level what completeness,
   soundness, and privacy (i.e., zero-knowledge) are.

   Things that are out of scope:

   *  Sybil attacks [Dou02]

   *  Differential privacy [Vad16]

7.  IANA Considerations

   This document has no IANA actions.

8.  References

8.1.  Normative References

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <https://datatracker.ietf.org/doc/html/rfc2119>.

   [RFC8174]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC
              2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174,
              May 2017, <https://datatracker.ietf.org/doc/html/rfc8174>.

8.2.  Informative References

   [AGJOP21]  Addanki, S., Garbe, K., Jaffe, E., Ostrovsky, R., and A.
              Polychroniadou, "Prio+: Privacy Preserving Aggregate
              Statistics via Boolean Shares", 2021,
              <https://ia.cr/2021/576>.

   [BBDGGI19] Boneh, D., Boyle, E., Corrigan-Gibbs, H., Gilboa, N., and
              Y. Ishai, "Zero-Knowledge Proofs on Secret-Shared Data via
              Fully Linear PCPs", CRYPTO 2019 , 2019.

   [BBDGGI21] Boneh, D., Boyle, E., Corrigan-Gibbs, H., Gilboa, N., and
              Y. Ishai, "Lightweight Techniques for Private Heavy
              Hitters", IEEE S&P 2021 , 2021.

   [CGB17]    Corrigan-Gibbs, H. and D. Boneh, "Prio: Private, Robust,
              and Scalable Computation of Aggregate Statistics", NSDI
              2017 , 2017.

   [Dou02]    Douceur, J., "The Sybil Attack", 2002,
              <https://link.springer.com/
              chapter/10.1007/3-540-45748-8_24>.

   [GI14]     Gilboa, N. and Y. Ishai, "Distributed Point Functions and
              Their Applications", EUROCRYPT 2014 , 2014.

   [PAPER]    "TODO", n.d..

   [Vad16]    Vadhan, S., "The Complexity of Differential Privacy",
              2016,
              <https://privacytools.seas.harvard.edu/files/privacytools/
              files/complexityprivacy_1.pdf>.

Acknowledgments

   TODO acknowledge.

Author's Address

   Christopher Patton
   Cloudflare

   Email: chrispatton+ietf@gmail.com
